{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QluQwa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "sv3kQDrCs4Td",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## FAQ\n",
        "* Как использовать: Рекомендуется использование платформы Google.Colab. Разместите в одной директории с файлом ноутбука файл весов, после чего поочередно запускайте фрагменты\n",
        "* Тестирование: Для тестирование выполните все фрагменты до Предварительных работ включительно, после чего запустите фрагмент Тестирование\n",
        "### Возможные проблемы:\n",
        "* Решение не набирает 200 очков: проверьте загрузились ли веса (если нет, то выдаст сообщение \"There isnt old weights\"), если веса загружены, то проверьте тип девайса (выводится под фрагментом №2), решение может набирать ~190 баллов, если решение запущено на CPU, в таком случае используйте  Google.Colab для запуска решения на GPU (cuda). Причина последней проблемы не известна, возможно проблема с преобразованием GPU весов в CUDA весов\n",
        "* \"IsLocked\" - перезапустите блок инициализации среды\n",
        "* \"There isnt old weights\" - файл с весами не найден, проверьте название файла и дирректорию в которой он находится\n",
        "* \"Your session crashed for an unknown reason\" - просто заново запустите каждый фрагмент\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DfetmoCPLpVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Double Deep Q-Network \n",
        "---\n",
        "\n",
        "\n",
        "## 1. Установка необходимых для работы в Google.Colab пакетов##"
      ]
    },
    {
      "metadata": {
        "id": "-Cy48641d1iX",
        "colab_type": "code",
        "outputId": "35b2c248-b1dc-4f53-8139-43a25e6221c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "cell_type": "code",
      "source": [
        "!apt install xvfb\n",
        "!apt install cuda\n",
        "\n",
        "!pip install box2d-py\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cuda is already the newest version (10.0.130-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.1.post2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c9wGSMpoACXp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2. Импортируем пакеты##\n",
        "В результате выполнения после блока будет выведен тип девайса на котором будет работать/тренироваться алгоритм. Если вы работаете в Google.Colab вы можете изменить тип девайса в вкладке Runtime -> Change Runtime Type\n"
      ]
    },
    {
      "metadata": {
        "id": "jb-quxNBLpVU",
        "colab_type": "code",
        "outputId": "b7b746d3-a145-423c-bb5b-2b44d38bf27a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import matplotlib\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display as ds\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_kkbexmUBPV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##3. Инициализируем среду##"
      ]
    },
    {
      "metadata": {
        "id": "GiXB6zOSLpVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gym.logger.set_level(40)\n",
        "\n",
        "env = gym.make('LunarLander-v2').unwrapped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sy1zqK2OBjgK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##4. Объявляем класс ReplayMemory, в котором будем хранить переходы из одного состояния в другое, действие, награду и состояние завершения эпизода##"
      ]
    },
    {
      "metadata": {
        "id": "S7ULk1OP0WXu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size, seed):\n",
        "\n",
        "        self.action_size = 4\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"next_state\", \"reward\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def push(self, state, action, next_state, reward, done):\n",
        "\n",
        "        e = self.experience(state, action, next_state, reward, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "  \n",
        "        return (states, actions, next_states, rewards, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ErayLLfDCKmS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##5. Объявляем класс нейросети##\n",
        "На входном слое объявляем 8 нейронов, на 2 скрытых по 64 и 4 класса на выходе согласно количеству действий"
      ]
    },
    {
      "metadata": {
        "id": "qXh7rXsLLpVl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()  \n",
        "        self.fc1 = nn.Linear(8, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc_out = nn.Linear(32, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = F.relu(self.fc3(out))\n",
        "        action = self.fc_out(out)\n",
        "        \n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-yeIbmEkCqw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##6. Предварительные работы ##\n",
        "Объявляем константы, 2 экземпляра нейросети, копируем веса, и один переводим в режим оценки. Объявляем функцию выбора действия, переводим основную нейросеть в режжим оценки, выбираем действие, после чего возвращаем в режим обучения."
      ]
    },
    {
      "metadata": {
        "id": "HPbw1RQfLpVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 100000\n",
        "GAMMA = .99\n",
        "EPS_START = .9\n",
        "EPS_END = .05\n",
        "EPS_DECAY = .995\n",
        "LR =0.0016\n",
        "\n",
        "policy_net = DQN().to(device)\n",
        "target_net = DQN().to(device)\n",
        "try:\n",
        "  policy_net.load_state_dict(torch.load(\"./model_mse_loss(super).nn\", map_location=device))\n",
        "except:\n",
        "  print(\"There isnt old weights\")\n",
        "  \n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "memory = ReplayMemory(buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, seed=0)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state, eps_threshold):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            policy_net.eval()\n",
        "            result = policy_net(state)\n",
        "            policy_net.train()\n",
        "            return np.argmax(result.cpu().data.numpy())\n",
        "            \n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
        "\n",
        "def test():\n",
        "    test_rewards = []\n",
        "    for i in range(100):\n",
        "        state = env.reset()\n",
        "        rewards=0\n",
        "        for j in count():\n",
        "            action = select_action(torch.FloatTensor(state).to(device), eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * i / EPS_DECAY))\n",
        "            state, reward, done, _ = env.step(action.item())\n",
        "            rewards+=reward\n",
        "    #         env.render()\n",
        "            if done:\n",
        "                break \n",
        "        test_rewards.append(rewards)       \n",
        "    env.close()\n",
        "    return np.mean(test_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eha2OT-lDqJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##7. Модель оптимизации (коррекции) весов нейросети\n",
        "Из памяти получаем пакет переходов, с помощью этих данных определяем Q_targets и Q_expected (целевое и ожидаемое значение Q функции), находим квадратичную ошибку и корректируем веса"
      ]
    },
    {
      "metadata": {
        "id": "Q9_YLL-iLpVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    \n",
        "    experiences = memory.sample()\n",
        "    states, actions, next_states, rewards, dones = experiences\n",
        "    \n",
        "    Q_argmax = target_net(torch.tensor(next_states).to(device)).detach()\n",
        "    _, a_prime = Q_argmax.max(1)\n",
        "\n",
        "    Q_targets_next = target_net(torch.tensor(next_states).to(device)).detach().gather(1, a_prime.unsqueeze(1))\n",
        "\n",
        "    Q_targets = torch.tensor(rewards).to(device) + (GAMMA * Q_targets_next * (1 - torch.tensor(dones).to(device)))\n",
        "\n",
        "    Q_expected = policy_net( torch.tensor(states).to(device)).gather(1, torch.tensor(actions).to(device))\n",
        "\n",
        "    loss = F.mse_loss(Q_targets, Q_expected)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ta6EbKeEmXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##8. Обучение\n",
        "Обучаем нейросеть и считаем среднее значение за каждые 100 эпизодов. Каждые 100 эпизодов сохраняем веса"
      ]
    },
    {
      "metadata": {
        "id": "dkqoGoGqLpVx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eps_threshold = EPS_START\n",
        "just_rewards = [0]\n",
        "lr=LR\n",
        "all_rewards = []\n",
        "for i_episode in count():\n",
        "      \n",
        "    state = env.reset()\n",
        "\n",
        "    rew_per_episode = 0\n",
        "    for t in count():\n",
        "          \n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "        action = select_action(torch.FloatTensor(state).to(device), eps_threshold)\n",
        "        next_state, env_reward, done, _ = env.step(action.item())\n",
        "\n",
        "        rew_per_episode += env_reward\n",
        "        if (t > 400) & (action.item()!=0):\n",
        "            env_reward-=0.3\n",
        "            \n",
        "        memory.push(state, action.item(), next_state, env_reward, done)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if len(memory) >= BATCH_SIZE:\n",
        "            optimize_model()\n",
        "        if done:\n",
        "            all_rewards.append(rew_per_episode)\n",
        "            just_rewards.append(rew_per_episode)\n",
        "            break\n",
        "        if(t%4==0):\n",
        "          target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print('\\rEpisode {} \\tLearning rate {:.6f} \\tAverage Score: {:.2f}'.format(i_episode+1, lr, np.mean(just_rewards)), end=\"\")\n",
        "    if((i_episode+1)%100==0):\n",
        "        print('\\rEpisode {} \\tLearning rate {:.6f} \\tAverage Score: {:.2f}'.format(i_episode+1, lr, np.mean(just_rewards)))\n",
        "        just_rewards = []\n",
        "        lr=LR/(i_episode+1)*100\n",
        "        torch.save(policy_net.state_dict(), \"model_mse_loss.nn\")\n",
        "        optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "        if(np.mean(just_rewards)>=195):\n",
        "          test_count = 0\n",
        "          for test_i in range(3):\n",
        "              test_mark = test()\n",
        "              print(test_mark)\n",
        "              if(test_mark>=195):\n",
        "                  test_count+=1\n",
        "          if(test_count>2):\n",
        "              break\n",
        "env.close()\n",
        "plt.figure\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZjDbUrZ5urD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Тестирование"
      ]
    },
    {
      "metadata": {
        "id": "qsSWDGO3LpV0",
        "colab_type": "code",
        "outputId": "74088a44-2ebd-43a8-ec4e-05de05e51985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "tested = test()\n",
        "print('\\rAverage score is: {:.2f}'.format(tested))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rAverage score is: 223.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oT06ShPmvJGM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Позалипать на агента :3"
      ]
    },
    {
      "metadata": {
        "id": "VS7G0MB4ypS-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d5710348-e4e6-4b0a-b21f-c204cab715bf"
      },
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "for j in count():\n",
        "    action = select_action(torch.FloatTensor(state).to(device), eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * i / EPS_DECAY))\n",
        "    img.set_data(env.render(mode='rgb_array')) \n",
        "    plt.axis('off')\n",
        "    ds.display(plt.gcf())\n",
        "    ds.clear_output(wait=True)\n",
        "    state, reward, done, _ = env.step(action.item())\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}